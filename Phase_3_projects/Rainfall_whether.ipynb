{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9469598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a09a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rainfall.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921299e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a46a72",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "- 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm' are Float in nature.\n",
    "- 'Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow' are Object in nature.\n",
    "- 'Rainfall' and 'RainTomorrow' are Target Variable. \n",
    "- This dataset contain 8425 Rows and 23 Columns.\n",
    "\n",
    "\n",
    "# Statistical Analysis\n",
    "\n",
    "**Since dataset is large, Let check for any entry which is repeated or duplicated in dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bafcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399743d4",
   "metadata": {},
   "source": [
    "**If we just check CSV File we can find that there are some missing value in dataset which shown fill with '?'**\n",
    "\n",
    "Let's check how many question mark (\"?\") inside dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([' ?']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5109e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([' ','NA','-']).sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea75cfe",
   "metadata": {},
   "source": [
    "- We have No whitespace, NA, '-', exist in dataset.\n",
    "\n",
    "**Lets drop duplicated entry from dataset before checking null values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4830bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88414d6",
   "metadata": {},
   "source": [
    "# Missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b52ce4",
   "metadata": {},
   "source": [
    "- All columns have missing values except Date and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75451e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6994064",
   "metadata": {},
   "source": [
    "**As missing values present are less than 1%. So we can directly drop these missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datatype of date column\n",
    "df['Date'] = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39289005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbb5d8",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- There 6762 rows in this dataset.\n",
    "- 16 columns are Numerical variable and having float64 datatypes.\n",
    "- 6 columns are categorical feature with object datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf81cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numerical and categorical variable\n",
    "Numerical = ['MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm']\n",
    "Categorical = ['Location','WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputating Missing value with mode for categorical features\n",
    "df['WindDir9am'].fillna(df['WindDir9am'].mode()[0],inplace=True)\n",
    "df['WindGustDir'].fillna(df['WindGustDir'].mode()[0],inplace=True)\n",
    "df['WindDir3pm'].fillna(df['WindDir3pm'].mode()[0],inplace=True)\n",
    "df['RainToday'].fillna(df['RainToday'].mode()[0],inplace=True)\n",
    "df['RainTomorrow'].fillna(df['RainTomorrow'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8365303",
   "metadata": {},
   "source": [
    "# Missing Value check After Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f293f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Numerical:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        val = df[col].mean()\n",
    "        df[col] = df[col].fillna(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af02a9",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "**Finally, No Missing Value is Present.**\n",
    "\n",
    "We are Now Yes To Go Further!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4f6cc",
   "metadata": {},
   "source": [
    "# Statistical Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Minimum Rainfall :'+\"\\033[0m\",df.Rainfall.min())\n",
    "print(\"\\033[1m\"+'Maximum rainfall :'+\"\\033[0m\",df.Rainfall.max())\n",
    "print(\"\\033[1m\"+'Average Rainfall :'+\"\\033[0m\",df.Rainfall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.title('Distribution Rainfall')\n",
    "sns.distplot(df[\"Rainfall\"], color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d061b50",
   "metadata": {},
   "source": [
    "- Most of cases Rainfall varies between 0 to 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f260dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('RainTomorrow')['Rainfall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b4eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage difference of Rain :'+\"\\033[0m\",((7.050487-1.486704)/1.486704)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(y=\"RainTomorrow\", x=\"Rainfall\", data=df, palette = 'hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Yes','No',\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(df.groupby('RainTomorrow')['Pressure9am'].mean(),labels = labels,radius =2,autopct = '%2.2f%%',explode=[0.3,0.2], shadow=True,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('RainTomorrow')['Pressure9am'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([df.RainTomorrow,df.Pressure9am],df.Rainfall, margins= True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Categorical:\n",
    "    print(i)\n",
    "    print(df[i].value_counts())\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"]=True\n",
    "sns.set_palette(\"husl\")\n",
    "f,ax=plt.subplots(1,2,figsize=(16,10))\n",
    "df['Rainfall'].value_counts().plot.pie(autopct='%3.1f%%', textprops={'fontweight':'bold','fontsize':18}, ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Population Distribution', fontsize=22, fontweight='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot(x='RainTomorrow', data=df, ax=ax[1])\n",
    "ax[1].set_title('Rainfall Distribution', fontsize=22,fontweight='bold')\n",
    "ax[1].set_xlabel('Rain',fontsize=18, fontweight='bold')\n",
    "plt.xticks(fontsize=18,fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf33dc",
   "metadata": {},
   "source": [
    "**Observation :**\n",
    "- 64.1% population have 0.0 rainfall\n",
    "- Our task is to predict rainTomorrow and we see that target variable rain is imbalanced.\n",
    "\n",
    "**Exploration of RainTomorrow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum :', df['Rainfall'].min())\n",
    "print('Maximum :', df['Rainfall'].max())\n",
    "print('Average :', df['Rainfall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb410425",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['WindGustDir'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution ', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['WindGustDir'],df[\"Rainfall\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['WindDir3pm'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution ', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc691ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['WindDir3pm'],df[\"Rainfall\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b165ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Cloud3pm'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution ', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19015751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Cloud3pm'],df[\"Rainfall\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['RainTomorrow'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution ', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['RainTomorrow'],df[\"Rainfall\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the pairwise relation in the dataset.\n",
    "sns.pairplot(df, hue=\"Rainfall\",palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd99f93",
   "metadata": {},
   "source": [
    "# Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29979746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in Categorical:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232185c",
   "metadata": {},
   "source": [
    "# Feature selection and Engineering\n",
    "\n",
    "# 1. Outliers Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10),facecolor='white')\n",
    "plotnumber=1\n",
    "\n",
    "for column in Numerical:\n",
    "    if plotnumber<=16:\n",
    "        ax=plt.subplot(6,3,plotnumber)\n",
    "        sns.boxplot(x=df[column],color='g')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a399f",
   "metadata": {},
   "source": [
    "**From Boxplot we can see outliers exist dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "df.drop(['Sunshine','Evaporation','WindGustSpeed'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08071cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d63564",
   "metadata": {},
   "source": [
    "**Outliers removal using Zscore method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "threshold = 3\n",
    "df1 = df[(z<3).all(axis = 1)]\n",
    "\n",
    "print (\"Shape of the dataframe before removing outliers: \", df.shape)\n",
    "print (\"Shape of the dataframe after removing outliers: \", df1.shape)\n",
    "print (\"Percentage of data loss post outlier removal: \", (df.shape[0]-df1.shape[0])/df.shape[0]*100)\n",
    "\n",
    "df=df1.copy() # reassigning the changed dataframe name to our original dataframe name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabc547",
   "metadata": {},
   "source": [
    "# 2. Skewness of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,10),facecolor='white')\n",
    "plotnum=1\n",
    "for col in df:\n",
    "    if plotnum<=19:\n",
    "        plt.subplot(5,4,plotnum)\n",
    "        sns.distplot(df[col],color='r')\n",
    "        plt.xlabel(col,fontsize=20)\n",
    "    plotnum+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eea46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b9946",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- Location, RainToday,RainTomorrow,WindDir9am,WindDir3pm are skewed but as they are categorical concept of skewness doesnot mean anything to it.\n",
    "- Rainfall are numerical variable with lot of zero and high number. So skewness exist in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082908d",
   "metadata": {},
   "source": [
    "# 3.Corrleation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44543db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bf587",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "df.corr()['Rainfall'].drop(['Rainfall']).sort_values(ascending=False).plot(kind='bar',color = 'purple')\n",
    "plt.xlabel(df,fontsize=15)\n",
    "plt.ylabel('Income',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable Income',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e682122",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- Temp9am, Location, WindSpeed9am, WindSpeed3pm are correlated with Target variable with less than 10% correlation. After checking Multicollinearity we will decide to drop these poorly correlated features or go for PCA.\n",
    "- RainToday is highly correlated with Target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0932d",
   "metadata": {},
   "source": [
    "# 4. Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36806e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n",
    "vif['Features']= df.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad9c5c",
   "metadata": {},
   "source": [
    "**Strategy to Address Multicollinearity:**\n",
    "- Removing Some of highly correlated features. But this will not work here as most of input features are correlated with each other moderated or poorly.\n",
    "- Another way to address Multicollinearity is to scaled data and then apply PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b536934",
   "metadata": {},
   "source": [
    "# 5.Balancing Imbalanced target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40728f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RainTomorrow.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1f7bd",
   "metadata": {},
   "source": [
    "**As Target variable data is imbalanced in nature we will need to balance target variable.**\n",
    "\n",
    "**Balancing using SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce588c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdebc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in target and dependent feature\n",
    "X = df.drop(['RainTomorrow'], axis =1)\n",
    "Y = df['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Oversampleing using SMOTE Techniques\n",
    "oversample = SMOTE()\n",
    "X, Y = oversample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec34b7f",
   "metadata": {},
   "source": [
    "**We have successfully resolved the class imbalanced problem and now all the categories have same data ensuring that the ML model does not get biased towards one category.**\n",
    "\n",
    "\n",
    "# Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f741f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "#plot the graph to find the principal components\n",
    "x_pca = pca.fit_transform(X_scale)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.title('Explained variance Ratio')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b288d",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "**AS per the graph, we can see that 9 principal components attribute for 90% variation in the data. We shall pick the first 9 components for our prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_new = PCA(n_components=9)\n",
    "x_new = pca_new.fit_transform(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "principle_x=pd.DataFrame(x_new,columns=np.arange(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b65842",
   "metadata": {},
   "source": [
    "# Machine learning model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a882737",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=99, test_size=.3)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,200):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(principle_x,Y,test_size = 0.3, random_state=i)\n",
    "    log_reg=LogisticRegression()\n",
    "    log_reg.fit(X_train,Y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    acc=accuracy_score(Y_test,y_pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is', maxAccu ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb560ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=44, test_size=.3)\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "y_pred=log_reg.predict(X_test)\n",
    "print('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\n",
    "print('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Optimal value of n_neighbors for KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(15):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train,Y_train)  #fit the model\n",
    "    y_pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2db7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values -\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(15), rmse_val, color='blue', linestyle='dashed', marker='o', markerfacecolor='green', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc166ebb",
   "metadata": {},
   "source": [
    "**Comment -**\n",
    "- At k=1, we get the minimum RMSE value which approximately 0.36647465376087945, and shoots up on further increasing the k value. We can safely that k=1 will give us the best result in this case.\n",
    "\n",
    "**Applying other classification algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18652e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=[ LogisticRegression(),\n",
    "        SVC(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 1),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier()]\n",
    "        \n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    y_pred=m.predict(X_test)\n",
    "    print('\\033[1m'+'Classification ML Algorithm Evaluation Matrix',m,'is' +'\\033[0m')\n",
    "    print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n",
    "    print('\\033[1m'+'Confusion matrix :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "    print('\\033[1m'+'Classification Report :'+'\\033[0m \\n',classification_report(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('=================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da707",
   "metadata": {},
   "source": [
    "# CrossValidation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=[LogisticRegression(),\n",
    "        SVC(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 1),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, principle_x, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n",
    "    print(\"Score :\" ,score)\n",
    "    print(\"Mean Score :\",score.mean())\n",
    "    print(\"Std deviation :\",score.std())\n",
    "    print('\\n')\n",
    "    print('============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac443b39",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedde322",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter= {'criterion' : ['gini', 'entropy'],\n",
    "              'min_samples_split':[3,5,8],\n",
    "              'max_depth' : [20,30,40],\n",
    "              'n_estimators' : [100, 150, 200]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd514a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(ExtraTreesClassifier(),parameter,verbose=2)\n",
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203fc38",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5970a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = ExtraTreesClassifier(criterion='entropy',n_estimators= 150, max_depth=40 ,min_samples_split= 3)\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38800a",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Rainfall_whether_forecast.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7575f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
